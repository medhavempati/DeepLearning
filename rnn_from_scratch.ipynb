{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6d2f1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8be551b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39533aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_sequences=2**8):\n",
    "    sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        token_length = np.random.randint(1, 12)\n",
    "        sequence = f'{\"a\"*token_length}{\"b\"*token_length}EOS'\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "790eac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_encoding(sequences):\n",
    "    \n",
    "    # Get 1D list of all words in all sequences\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Create dictionary mapping word to word frequency across all sequences\n",
    "    word_to_count = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        word_to_count[word] += 1\n",
    "    word_to_count = sorted(list(word_to_count.items()), key=lambda l: -l[1]) # sorting according to frequency\n",
    "    \n",
    "    # List of unique words\n",
    "    dictionary = [item[0] for item in word_to_count]\n",
    "    dictionary.append('UNK')\n",
    "    \n",
    "    # Calculate lengths\n",
    "    num_sequences = len(sequences)\n",
    "    vocab_size = len(dictionary)\n",
    "    \n",
    "    # Make word to index and index to word mappings\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "    for idx, word in enumerate(dictionary):\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "    \n",
    "    return word_to_idx, idx_to_word, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc1a2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2e970cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.X = inputs\n",
    "        self.y = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e637d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sequences, train_size=0.8, test_size=0.1, val_size=0.1):\n",
    "    \n",
    "    # Split data\n",
    "    num_train = int(train_size*len(sequences))\n",
    "    num_test = int(test_size*len(sequences))\n",
    "    num_val = int(val_size*len(sequences))\n",
    "#     print(f'{num_train}, {num_test}, {num_val}')\n",
    "    \n",
    "    train_seq = sequences[:num_train]\n",
    "    test_seq = sequences[num_train:num_train+num_test]\n",
    "    val_seq = sequences[-num_val:]\n",
    "#     print(f'{len(train_seq)}, {len(test_seq)}, {len(val_seq)}')\n",
    "    \n",
    "    # prepare input & target sequences\n",
    "    def prepare_sequences(sequences):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "        \n",
    "        return inputs, targets\n",
    "    \n",
    "    train_inputs, train_targets = prepare_sequences(train_seq)\n",
    "    test_inputs, test_targets = prepare_sequences(test_seq)\n",
    "    val_inputs, val_targets = prepare_sequences(val_seq)\n",
    "#     print(f'{len(train_inputs)}, {len(test_inputs)}, {len(val_inputs)}')\n",
    "    \n",
    "    # create datasets\n",
    "    train_set = Dataset(train_inputs, train_targets)\n",
    "    test_set = Dataset(test_inputs, test_targets)\n",
    "    val_set = Dataset(val_inputs, val_targets)\n",
    "    \n",
    "    return train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "863fd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ccb7df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_orthogonal_weights(dim1, dim2):\n",
    "    \n",
    "    # initialize\n",
    "    weights = np.random.randn(dim1, dim2)\n",
    "#     print(f'inital random: {weights}')\n",
    "    if dim1 < dim2:\n",
    "        weights = weights.T\n",
    "        \n",
    "    # QR factorization (Q = orthogonal)\n",
    "    q, r = np.linalg.qr(weights)\n",
    "#     print(f'q: {q}')\n",
    "    \n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "#     print(f'q final: {q}')\n",
    "\n",
    "    if dim1 < dim2:\n",
    "        q = q.T\n",
    "        \n",
    "    return q # q is orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "107d38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn(hidden_size, vocab_size):\n",
    "    '''\n",
    "    Initializes RNN\n",
    "    \n",
    "    Args:\n",
    "        hidden_size --> hidden state dimensions\n",
    "        vocab_size --> input vector dimensions\n",
    "\n",
    "    Returns:\n",
    "        U --> Weight matrix applied to input, passed to hidden state\n",
    "        V --> Weight matrix from previous hidden state passed to hidden state\n",
    "        W --> Weight matrix applied to output from hidden state to give final output\n",
    "\n",
    "        bias_hidden = bias applied in hidden state\n",
    "        bias_output = bias applied to output\n",
    "    '''\n",
    "    \n",
    "    U = init_orthogonal_weights(hidden_size, vocab_size)\n",
    "    V = init_orthogonal_weights(hidden_size, hidden_size)\n",
    "    W = init_orthogonal_weights(vocab_size, hidden_size)\n",
    "    \n",
    "    bias_hidden = init_orthogonal_weights(hidden_size, hidden_size)\n",
    "    bias_output = init_orthogonal_weights(vocab_size, vocab_size)\n",
    "    \n",
    "    return (U, V, W, bias_hidden, bias_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25bbbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf45141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes sigmoid of array x\n",
    "    \n",
    "    Args:\n",
    "        x --> input array\n",
    "        derivative --> when set to True will return derivative instead of forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: \n",
    "        return f * (1 - f)\n",
    "    else: \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e5c9d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes tanh of array x\n",
    "    \n",
    "    Args:\n",
    "        x --> input array\n",
    "        derivative --> when set to True will return derivative instead of forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: \n",
    "        return f * (1 - f)\n",
    "    else: \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a258003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes softmax of array x\n",
    "    \n",
    "    Args:\n",
    "        x --> input array\n",
    "    \"\"\"\n",
    "    return np.exp(x+1e-12) / np.sum(np.exp(x+1e-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4295facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c9df1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hidden_state, parameters):\n",
    "    \n",
    "    U, V, W, bias_hidden, bias_output = parameters\n",
    "    outputs, hidden_states = [], [hidden_state]\n",
    "#     print(f'U: {U}, V: {V}, W: {W}')\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        \n",
    "#         print(f'U: {U.shape}, input: {inputs[i].shape}, v: {V.shape}, hidden: {hidden_state.shape}')\n",
    "        hidden_state = tanh((np.dot(U, inputs[i]) + np.dot(V, hidden_states[-1])))\n",
    "        output = np.dot(W, hidden_state)\n",
    "#         print(f'hidden: {hidden_state>0}, output: {output}')\n",
    "        \n",
    "        hidden_states.append(hidden_state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e94c2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Prevents exploding gradient by clipping \n",
    "    Clips gradients to have max norm of max_norm\n",
    "    \"\"\"\n",
    "\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "\n",
    "    # Using L2 norm squared\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d759e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(output, target):\n",
    "    loss = 0\n",
    "    for j in range(len(output)):\n",
    "        \n",
    "#         print(f'target: {target[j]}, out: {output[j]}, val: {output[j]}, log: {np.log(output[j] + 1e-9)}')\n",
    "        loss += target[j] * np.log(output[j] + 1e-9) \n",
    "    \n",
    "    return -loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "02d421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
    "    U, V, W, bias_hidden, bias_output = params\n",
    "    \n",
    "    # Initialize gradients as zero\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_bias_hidden, d_bias_output = np.zeros_like(bias_hidden), np.zeros_like(bias_output)\n",
    "    \n",
    "    d_hidden_next = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    # Iterate backwards through elements\n",
    "    for i in reversed(range(len(outputs))):\n",
    "        \n",
    "        # Calculate loss\n",
    "#         print(f'{cross_entropy_loss(outputs[i], targets[i])}')\n",
    "        loss += (cross_entropy_loss(softmax(outputs[i]), targets[i])/len(targets))\n",
    "        \n",
    "        # Backpropagate into output\n",
    "        d_output = outputs[i].copy()\n",
    "        d_output[np.argmax(targets[i])] -= 1\n",
    "        \n",
    "        # Backpropagate into W\n",
    "#         print(f'h: {hidden_states[i].T.shape}, out: {d_output.shape}')\n",
    "        d_W += np.dot(d_output, hidden_states[i].T)\n",
    "        d_bias_output += d_output\n",
    "        \n",
    "        # Backpropagate into h\n",
    "        d_h = np.dot(W.T, d_output) + d_hidden_next\n",
    "        \n",
    "        # Backpropagate through non-linearity (tanh)\n",
    "        d_f = (1 - hidden_states[i]**2) * d_h\n",
    "        d_bias_hidden += d_f\n",
    "        \n",
    "        # Backpropagate into U\n",
    "#         print(f'h: {inputs[i].T.shape}, out: {d_f.shape}')\n",
    "        d_U += np.dot(d_f, inputs[i].T)\n",
    "        \n",
    "        # Backpropagate into V\n",
    "#         print(f'h: {hidden_states[i-1].T.shape}, out: {d_f.shape}')\n",
    "        d_V += np.dot(hidden_states[i-1].T, d_f)\n",
    "        d_hidden_next = np.dot(V.T, d_f)\n",
    "        \n",
    "    # Clip gradients\n",
    "    grads = d_U, d_V, d_W, d_bias_hidden, d_bias_output\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "22b20dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(parameters, gradients, learning_rate=1e-3):\n",
    "    for parameter, gradient in zip(parameters, gradients):\n",
    "        parameter -= learning_rate * gradient\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2c17f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(dataset, vocab_size):\n",
    "    \n",
    "    x, y = [], []\n",
    "    for inputs, targets in dataset:\n",
    "#         print(f'input: {len(inputs)}\\ntargets{len(targets)}\\n')\n",
    "        x.append(one_hot_encode_sequence(inputs, vocab_size))\n",
    "        y.append(one_hot_encode_sequence(targets, vocab_size))\n",
    "        \n",
    "#     print(f'lengths {len(x)}, {len(y)}')\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "76c55695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_set, hidden_state, parameters, epochs=1000):\n",
    "    \n",
    "    training_loss = []\n",
    "    inputs, targets = training_set\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        epoch_training_loss = 0\n",
    "        for x, y in zip(inputs, targets):\n",
    "            hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, hidden_states = forward_pass(x, hidden_state, parameters)\n",
    "\n",
    "            # Backward pass\n",
    "            loss, gradients = backward_pass(x, outputs, hidden_states, y, parameters)\n",
    "            if np.isnan(loss):\n",
    "                raise ValueError('ERROR: Gradients have vanished')\n",
    "\n",
    "            # Update parameters (optimizer)\n",
    "            parameters = optimizer(parameters, gradients)\n",
    "            epoch_training_loss += loss\n",
    "            \n",
    "        training_loss.append(epoch_training_loss/len(training_set))\n",
    "    \n",
    "        if i%100 == 0:\n",
    "            print(f'Epoch {i}, training loss: {training_loss[-1]}')\n",
    "        \n",
    "    return parameters, training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "81e3bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_set, hidden_state, parameters, epochs=100):\n",
    "    \n",
    "    validation_loss = []\n",
    "    inputs, targets = val_set\n",
    "    for i in range(epochs):\n",
    "        epoch_validation_loss = 0\n",
    "        for x, y in zip(inputs, targets):\n",
    "            hidden_state = np.zeros_like(hidden_state)\n",
    "            \n",
    "            #Forward pass\n",
    "            outputs, hidden_states = forward_pass(x, hidden_state, parameters)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss, _ = backward_pass(x, outputs, hidden_states, y, parameters)\n",
    "            if np.isnan(loss):\n",
    "                raise ValueError('ERROR: Gradients have vanished')\n",
    "            \n",
    "        validation_loss.append(epoch_validation_loss/len(val_set))\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(f'Epoch {i}, validation loss: {validation_loss[-1]}')\n",
    "            \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8722d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_set, hidden_state, parameters, idx_to_word):\n",
    "    inputs, targets = test_set\n",
    "    results = defaultdict()\n",
    "    for x in inputs:\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "        outputs, hidden_states = forward_pass(x, hidden_state, parameters)\n",
    "        x_decoded = [ind_to_word[np.argmax(x[i])] for i in range(len(x))]\n",
    "        y_decoded = [ind_to_word[np.argmax(output)] for output in outputs]\n",
    "        x_decoded = ('').join(x_decoded)\n",
    "        y_decoded = ('').join(y_decoded)\n",
    "        results[x_decoded] = y_decoded\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0b9d1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn():\n",
    "    \n",
    "    # Constants\n",
    "    epochs = 100\n",
    "    hidden_size = 50\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    # Data Preparation\n",
    "    sequences = generate_dataset()\n",
    "    word_to_idx, idx_to_word, vocab_size = word_encoding(sequences)\n",
    "    train_set, test_set, val_set = prepare_data(sequences)\n",
    "    \n",
    "    # Data encoding\n",
    "    train_set = encode_data(train_set, vocab_size)\n",
    "    test_set = encode_data(test_set, vocab_size)\n",
    "    val_set = encode_data(val_set, vocab_size)\n",
    "    \n",
    "    # Initialize rnn\n",
    "    parameters = init_rnn(hidden_size, vocab_size)\n",
    "    training_loss, validation_loss = [], []\n",
    "    \n",
    "    # Train\n",
    "    parameters, training_loss = train(train_set, hidden_state, parameters, epochs)\n",
    "    \n",
    "    # Validate\n",
    "    validation_loss = validate(val_set, hidden_state, parameters, epochs)\n",
    "    \n",
    "    # Test\n",
    "    results = test(test_set, hidden_state, parameters, idx_to_word)\n",
    "    \n",
    "    # Print results\n",
    "    for key in results:\n",
    "        print(f'Input: {key}, Output: {results[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5e65dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204, 25, 25\n",
      "204, 25, 25\n",
      "204, 25, 25\n",
      "Epoch 0, training loss: [190.37475193]\n",
      "Epoch 0, validation loss: 0.0\n",
      "Input: aaaaaaaaabbbbbbbbbEO, Output: aaabbbbbbbbbbbbbbbbb\n",
      "Input: aaabbbEO, Output: aaabbbbb\n",
      "Input: aaaaaaaaaabbbbbbbbbbEO, Output: aaabbbbbbbbbbbbbbbbbbb\n",
      "Input: aaaaaaaaaaabbbbbbbbbbbEO, Output: aaabbbbbbbbbbbbbbbbbbbbb\n",
      "Input: aabbEO, Output: aabbbS\n",
      "Input: abEO, Output: abbS\n",
      "Input: aaaaabbbbbEO, Output: aaabbbbbbbbb\n",
      "Input: aaaaaaaabbbbbbbbEO, Output: aaabbbbbbbbbbbbbbb\n",
      "Input: aaaaaabbbbbbEO, Output: aaabbbbbbbbbbb\n",
      "Input: aaaaaaabbbbbbbEO, Output: aaabbbbbbbbbbbbb\n"
     ]
    }
   ],
   "source": [
    "rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "723b5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac5382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
